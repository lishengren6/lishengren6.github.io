<!DOCTYPE html>
<html>
<head>
  <title> Lisheng Ren Homepage </title>
  <meta name="description"
  content="I am a PhD student in the CS department at UW-Madison, advised by Ilias Diakonikolas. I am working on Theoretical Computer Science and Machine Learning.">
  <meta name="keywords"
  content="Lisheng, Ren,  Graduate Student, Computer Science, Machine Learning, UW-Madison, UW, Theoretical, Learning Theory, Ilias Diakonikolas">
  <style type="text/css">
    body{
      margin:20px auto;
      max-width:800px;
      line-height:1.6;
      font-size:15px;
      /*font-family: 'Noto Sans', sans-serif;*/
      font-family: 'Roboto', sans-serif;
      color:#000;
      padding:10px
    }
  a:link {text-decoration:none; color:#1A73E8;}
  a:visited {text-decoration:none; color:#1A73E8;}
	.abstract_link { cursor: pointer; text-decoration: none; color:#1A73E8; font-size:15px;}
	.abstract {width:700px; font-size:10pt; text-align:justify; margin-top:10px;}
  </style>

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
  <script>
  function reveal_abstract(id) {
    var element = document.getElementById(id);
    var style = element.style;
    if (style.display == "none") {
      style.display = "block";
    } else {
      style.display = "none";
    }
  }
  </script>
<head>

<body>
  <h1>Lisheng Ren</h1>
  <p>
    I am a PhD student in <a href="https://www.cs.wisc.edu/" target="_blank">Department of Computer Sciences</a> at University of Wisconsin-Madison,
    where I am fortunate to be advised by Professor <a href="http://www.iliasdiakonikolas.org/" target="_blank">Ilias Diakonikolas</a>.
    Before coming to Madison, I did my undergraduate study at <a href="https://science.rpi.edu/computer-science" target="_blank"> Rensselaer Polytechnic Institute</a>.
    I am interested in theoretical machine learning and algorithms.
  </p>

  <p>
    Feel free to contact me at: lishengren6[at]gmail.com.
  </p>

<h2 style="clear: both;">Publications</h2>
<ol>
  <li> <b> Cryptographic Hardness of Learning Halfspaces with Massart Noise </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper3abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2207.14266">[arxiv]</a>
  <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
       <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
       <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, and Lisheng Ren
  <br> <i> Advances in Neural Information Processing Systems (NeurIPS 2022) </i>
  <div id="paper3abs" style="display:none" class='abstract'>
  We study the complexity of PAC learning halfspaces in the presence of Massart noise.
  In this problem, we are given i.i.d. labeled examples
  $(\mathbf{x}, y) \in \mathbb{R}^N \times \{ \pm 1\}$,
  where the distribution of $\mathbf{x}$ is arbitrary
  and the label $y$ is a Massart corruption
  of $f(\mathbf{x})$, for an unknown halfspace $f: \mathbb{R}^N \to \{ \pm 1\}$,
  with flipping probability $\eta(\mathbf{x}) \leq \eta < 1/2$.
  The goal of the learner is to compute a hypothesis with small 0-1 error.
  Our main result is the first computational hardness result for this learning problem.
  Specifically, assuming the (widely believed) subexponential-time hardness
  of the Learning with Errors (LWE) problem, we show that no polynomial-time
  Massart halfspace learner can achieve error better than $\Omega(\eta)$,
  even if the optimal 0-1 error is small, namely
  ${\rm OPT}  =  2^{-\log^{c} (N)}$
  for any universal constant $c \in (0, 1)$.
  Prior work had provided qualitatively similar evidence of hardness in the
  Statistical Query model. Our computational hardness result
  essentially resolves the polynomial PAC learnability of Massart halfspaces,
  by showing that known efficient learning algorithms
  for the problem are nearly best possible.
  </div>
  </li>

  <br>
  <li> <b> SQ Lower Bounds for Learning Single Neurons with Massart Noise </b>
    <a class= 'abstract_link' onclick= "reveal_abstract('paper2abs')">[abstract]</a>
    <a href="https://arxiv.org/abs/2210.09949">[arxiv]</a>
    <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
      <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
      Lisheng Ren,
      and
      <a href="https://pages.cs.wisc.edu/~yxsun/">Yuxin Sun</a>
    <br> <i> Advances in Neural Information Processing Systems (NeurIPS 2022) </i>
    <div id="paper2abs" style="display:none" class='abstract'>
    We study the problem of PAC learning a single neuron in the presence of Massart noise. 
    Specifically, for a known activation function $f: \mathbb{R} \to \mathbb{R}$, the learner is given
    access to labeled examples $({\bf x}, y) \in \mathbb{R}^d \times \mathbb{R}$, where the marginal distribution of ${\bf x}$ is
    arbitrary and the corresponding label $y$ is a Massart corruption of $f(\langle {\bf w}, {\bf x} \rangle)$.
    The goal of the learner is to output a hypothesis $h: \mathbb{R}^d \to \mathbb{R}$ with small squared loss.
    For a range of activation functions, including ReLUs,
    we establish super-polynomial Statistical Query (SQ) lower bounds for this learning problem.
    In more detail, we prove that no efficient SQ algorithm can approximate
    the optimal error within any constant factor.
    Our main technical contribution is a novel SQ-hard construction
    for learning $\{ \pm 1\}$-weight Massart halfspaces on the Boolean hypercube
    that is interesting on its own right.
    </div>
  </li>

  <br>
  <li> <b> Hardness of Learning a Single Neuron with Adversarial Label Noise</b>
  	<br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
         <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
         <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, and Lisheng Ren
  	<br> <i> Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022)</i>
    <br> Selected for <b> Oral Presentation </b>
  </li>
</ol>

<h2 style="clear: both;">Teaching and Service</h2>
<ul style="list-style-type: none">
  <li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, Spring 2022</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Algorithms, UW-Madison, Spring 2021</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, Fall 2020</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Computer Engineering, UW-Madison, Fall 2019</li>
  <li style="margin: 10px 0"><b>Reviewer:</b> AISTATS 2022</li>
</ul>
</body>
<html>
