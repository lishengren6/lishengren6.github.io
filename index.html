<!DOCTYPE html>
<html>
<head>
  <title> Lisheng Ren Homepage </title>
  <meta name="description"
  content="I am a PhD student in the CS department at UW-Madison, advised by Ilias Diakonikolas. I am working on Theoretical Computer Science and Machine Learning.">
  <meta name="keywords"
  content="Lisheng, Ren,  Graduate Student, Computer Science, Machine Learning, UW-Madison, UW, Theoretical, Learning Theory, Ilias Diakonikolas">
  <style type="text/css">
    body{
      margin:20px auto;
      max-width:800px;
      line-height:1.6;
      font-size:15px;
      /*font-family: 'Noto Sans', sans-serif;*/
      font-family: 'Roboto', sans-serif;
      color:#000;
      padding:10px
    }
  a:link {text-decoration:none; color:#1A73E8;}
  a:visited {text-decoration:none; color:#1A73E8;}
	.abstract_link { cursor: pointer; text-decoration: none; color:#1A73E8; font-size:15px;}
	.abstract {width:700px; font-size:10pt; text-align:justify; margin-top:10px;}
  </style>

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
  <script>
  function reveal_abstract(id) {
    var element = document.getElementById(id);
    var style = element.style;
    if (style.display == "none") {
      style.display = "block";
    } else {
      style.display = "none";
    }
  }
  </script>
<head>

<body>
  <h1>Lisheng Ren</h1>
  <p>
    I am a PhD student in <a href="https://www.cs.wisc.edu/" target="_blank">Department of Computer Sciences</a> at University of Wisconsin-Madison,
    where I am fortunate to be advised by Professor <a href="http://www.iliasdiakonikolas.org/" target="_blank">Ilias Diakonikolas</a>.
    Before coming to Madison, I did my undergraduate study at <a href="https://science.rpi.edu/computer-science" target="_blank"> Rensselaer Polytechnic Institute</a>.
    I am interested in theoretical machine learning and algorithms.
  </p>

  <p>
    Feel free to contact me at: lishengren6[at]gmail.com.
  </p>

<h2 style="clear: both;">Publications</h2>
<ol>
  <li> <b> Faster Algorithms for Agnostically Learning Disjunctions and their Implications </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper10abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2504.15244">[arxiv]</a>
  <br>
    <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a> and
    Lisheng Ren
  <br> <i> Conference on Learning Theory (COLT 2025) </i>
  <br> <b> Best Student Paper Award </b>
  <div id="paper10abs" style="display:none" class='abstract'>
    We study the algorithmic task of learning Boolean disjunctions
    in the distribution-free agnostic PAC model. The best known
    agnostic learner for the class of disjunctions over $\{0, 1\}^n$
    is the $L_1$-polynomial regression algorithm,
    achieving complexity $2^{\tilde{O}(n^{1/2})}$.
    This complexity bound is known to be nearly best possible
    within the class of Correlational Statistical Query (CSQ) algorithms.
    In this work, we develop an agnostic learner
    for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$.
    Our algorithm can be implemented in the Statistical Query (SQ) model,
    providing the first separation between the SQ and CSQ models in
    distribution-free agnostic learning.
  </div>
  </li>

  <br>
  <li> <b> Learning Intersections of Two Margin Halfspaces under Factorizable Distributions </b>
  <br>
    <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    <a href="https://mmingchen.github.io/">Mingchen Ma</a>,
    Lisheng Ren,
    and
    <a href="https://tzamos.com/">Christos Tzamos</a>
  <br> <i> Conference on Learning Theory (COLT 2025) </i>
  <div id="paper9abs" style="display:none" class='abstract'>
  </div>
  </li>

  <br>
  <li> <b> Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper8abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2502.11413">[arxiv]</a>
  <br>
    <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    <a href="https://mmingchen.github.io/">Mingchen Ma</a>,
    Lisheng Ren,
    and
    <a href="https://tzamos.com/">Christos Tzamos</a>
  <br> <i> Advances in International Conference on Machine Learning (ICML 2025) </i>
  <br> Selected for <b> Oral Presentation </b>
  <div id="paper8abs" style="display:none" class='abstract'>
    We study the task of Multiclass Linear Classification (MLC)
    in the distribution-free PAC model
    with Random Classification Noise (RCN).
    Specifically, the learner is given a set of
    labeled examples $(x, y)$, where $x$ is drawn
    from an unknown distribution on $\mathbb{R}^d$
    and the labels are generated by a
    multiclass linear classifier corrupted with RCN.
    That is, the label $y$ is flipped from $i$ to $j$
    with probability $H_{ij}$
    according to a known noise matrix $H$ with
    non-negative separation
    $\sigma: = \min_{i \neq j} H_{ii}-H_{ij}$.
    The goal is to compute a hypothesis with
    small 0-1 error. For the special case of two labels,
    prior work has given polynomial-time algorithms
    achieving the optimal error.
    Surprisingly, little is known about
    the complexity of this task even for three labels.
    As our main contribution, we show that the complexity
    of MLC with RCN becomes drastically different
    in the presence of three or more labels.
    Specifically, we prove {\em super-polynomial}
    Statistical Query (SQ) lower bounds for this problem.
    In more detail, even for three labels and
    constant separation,
    we give a super-polynomial lower bound
    on the complexity of any SQ algorithm achieving optimal error.
    For a larger number of labels  and smaller separation,
    we show a super-polynomial SQ lower bound even
    for the weaker goal of achieving {\em any} constant
    factor approximation to the optimal loss or even beating the trivial hypothesis.
  </div>
  </li>

  <br>
  <li> <b> Reliable Learning of Halfspaces under Gaussian Marginals </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper7abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2411.11238">[arxiv]</a>
  <br>
    <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    Lisheng Ren,
    and
    <a href="https://nikoszarifis.github.io/">Nikos Zarifis</a>
  <br> <i> Advances in Neural Information Processing Systems (NeurIPS 2024) </i>
  <br> Selected for <b> Spotlight Presentationn </b>
  <div id="paper7abs" style="display:none" class='abstract'>
    We study the problem of PAC learning halfspaces in the
    reliable agnostic model of Kalai et al. (2012).
    The reliable PAC model
    captures learning scenarios where one type of error is
    costlier than the others. Our main positive result is a
    new algorithm for reliable learning
    of Gaussian halfspaces on
    $\mathbb{R}^d$ with sample and computational complexity
    $$d^{O(\log (\min\{1/\alpha, 1/\epsilon\}))}\min (2^{\log(1/\epsilon)^{O(\log (1/\alpha))}},2^{\mathrm{poly}(1/\epsilon)})\;,$$
    where $\epsilon$ is the excess error and $\alpha$
    is the bias of the optimal halfspace. We complement our upper bound with
    a Statistical Query lower bound
    suggesting that the $d^{\Omega(\log (1/\alpha))}$ dependence is best possible.
    Conceptually, our results imply a strong computational separation
    between reliable agnostic learning and standard agnostic
    learning of halfspaces in the Gaussian setting.
  </div>
  </li>

  <br>
  <li> <b> Fast Co-Training under Weak Dependence via Stream-Based Active Learning </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper6abs')">[abstract]</a>
  <br>
    <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    <a href="https://mmingchen.github.io/">Mingchen Ma</a>,
    Lisheng Ren,
    and
    <a href="https://tzamos.com/">Christos Tzamos</a>
  <br> <i> Advances in International Conference on Machine Learning (ICML 2024) </i>
  <br> Selected for <b> Oral Presentation </b>
  <div id="paper6abs" style="display:none" class='abstract'>
  Co-training is a classical semi-supervised learning method which only requires a small number of labeled examples for learning,
  under reasonable assumptions. Despite extensive literature on the topic,
  very few hypothesis classes are known to be provably efficiently learnable via co-training,
  even under very strong distributional assumptions.
  In this work, we study the co-training problem in the stream-based active learning model.
  We show that a range of natural concept classes are efficiently learnable via co-training, in terms of both label efficiency and computational efficiency.

  We provide an efficient reduction of co-training under the standard assumption of weak dependence, in the stream-based active model, to online classification.
  As a corollary, we obtain efficient co-training algorithms with error independent label complexity for every concept class class efficiently learnable in the mistake bound online model.
  Our framework also gives co-training algorithms with label complexity $\tilde{O}(d\log (1/\epsilon))$ for any concept class with VC dimension $d$,
  though in general this reduction is not computationally efficient.
  Finally, using additional ideas from online learning, we design the first efficient co-training algorithms with label complexity $\tilde{O}(d^2\log (1/\epsilon))$ for several concept classes,
  including unions of intervals and homogeneous halfspaces.
  </div>
  </li>

  <br>
  <li> <b> SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper5abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2403.04744">[arxiv]</a>
  <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
    <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
    Lisheng Ren,
    and
    <a href="https://pages.cs.wisc.edu/~yxsun/">Yuxin Sun</a>
  <br> <i> Neural Information Processing Systems (NeurIPS 2023) </i>
  <div id="paper5abs" style="display:none" class='abstract'>
  We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model.
  Prior work developed a methodology to prove SQ lower bounds for NGCA that have been applicable to a wide range of contexts.
  In particular, it was known that for any univariate distribution $A$ satisfying certain conditions,
  distinguishing between a standard multivariate Gaussian and a distribution
  that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard.
  The required conditions were that (1) $A$ matches many low-order moments with a standard Gaussian,
  and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite.
  While the moment-matching condition is clearly necessary for hardness, the chi-squared condition was only required for technical reasons.
  In this work, we establish that the latter condition is indeed not necessary.
  In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only.
  </div>
  </li>

  <br>
  <li> <b> Near-Optimal Cryptographic Hardness of Agnostically Learning Halfspaces and ReLU Regression under Gaussian Marginals </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper4abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2302.06512">[arxiv]</a>
  <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
       <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>, and Lisheng Ren
  <br> <i> Advances in International Conference on Machine Learning (ICML 2023) </i>
  <div id="paper4abs" style="display:none" class='abstract'>
  We study the task of agnostically learning halfspaces under the Gaussian distribution.
  Specifically, given labeled examples $(\mathbf{x},y)$ from an unknown distribution on $\mathbb{R}^n \times \{\pm 1 \}$,
  whose marginal distribution on $\mathbf{x}$ is the standard Gaussian and the labels $y$ can be arbitrary,
  the goal is to output a hypothesis with 0-1 loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is the 0-1 loss of the best-fitting halfspace.
  We prove a near-optimal computational hardness result for this task, under the widely believed
  sub-exponential time hardness of the Learning with Errors (LWE) problem. Prior hardness results are either
  qualitatively suboptimal or apply to restricted families of algorithms. Our techniques extend to
  yield near-optimal lower bounds for related problems, including ReLU regression.
  </div>
  </li>

  <br>
  <li> <b> Cryptographic Hardness of Learning Halfspaces with Massart Noise </b>
  <a class= 'abstract_link' onclick= "reveal_abstract('paper3abs')">[abstract]</a>
  <a href="https://arxiv.org/abs/2207.14266">[arxiv]</a>
  <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
       <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
       <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, and Lisheng Ren
  <br> <i> Advances in Neural Information Processing Systems (NeurIPS 2022) </i>
  <div id="paper3abs" style="display:none" class='abstract'>
  We study the complexity of PAC learning halfspaces in the presence of Massart noise.
  In this problem, we are given i.i.d. labeled examples
  $(\mathbf{x}, y) \in \mathbb{R}^N \times \{ \pm 1\}$,
  where the distribution of $\mathbf{x}$ is arbitrary
  and the label $y$ is a Massart corruption
  of $f(\mathbf{x})$, for an unknown halfspace $f: \mathbb{R}^N \to \{ \pm 1\}$,
  with flipping probability $\eta(\mathbf{x}) \leq \eta < 1/2$.
  The goal of the learner is to compute a hypothesis with small 0-1 error.
  Our main result is the first computational hardness result for this learning problem.
  Specifically, assuming the (widely believed) subexponential-time hardness
  of the Learning with Errors (LWE) problem, we show that no polynomial-time
  Massart halfspace learner can achieve error better than $\Omega(\eta)$,
  even if the optimal 0-1 error is small, namely
  ${\rm OPT}  =  2^{-\log^{c} (N)}$
  for any universal constant $c \in (0, 1)$.
  Prior work had provided qualitatively similar evidence of hardness in the
  Statistical Query model. Our computational hardness result
  essentially resolves the polynomial PAC learnability of Massart halfspaces,
  by showing that known efficient learning algorithms
  for the problem are nearly best possible.
  </div>
  </li>

  <br>
  <li> <b> SQ Lower Bounds for Learning Single Neurons with Massart Noise </b>
    <a class= 'abstract_link' onclick= "reveal_abstract('paper2abs')">[abstract]</a>
    <a href="https://arxiv.org/abs/2210.09949">[arxiv]</a>
    <br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
      <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
      Lisheng Ren,
      and
      <a href="https://pages.cs.wisc.edu/~yxsun/">Yuxin Sun</a>
    <br> <i> Advances in Neural Information Processing Systems (NeurIPS 2022) </i>
    <div id="paper2abs" style="display:none" class='abstract'>
    We study the problem of PAC learning a single neuron in the presence of Massart noise.
    Specifically, for a known activation function $f: \mathbb{R} \to \mathbb{R}$, the learner is given
    access to labeled examples $({\bf x}, y) \in \mathbb{R}^d \times \mathbb{R}$, where the marginal distribution of ${\bf x}$ is
    arbitrary and the corresponding label $y$ is a Massart corruption of $f(\langle {\bf w}, {\bf x} \rangle)$.
    The goal of the learner is to output a hypothesis $h: \mathbb{R}^d \to \mathbb{R}$ with small squared loss.
    For a range of activation functions, including ReLUs,
    we establish super-polynomial Statistical Query (SQ) lower bounds for this learning problem.
    In more detail, we prove that no efficient SQ algorithm can approximate
    the optimal error within any constant factor.
    Our main technical contribution is a novel SQ-hard construction
    for learning $\{ \pm 1\}$-weight Massart halfspaces on the Boolean hypercube
    that is interesting on its own right.
    </div>
  </li>

  <br>
  <li> <b> Hardness of Learning a Single Neuron with Adversarial Label Noise</b>
  	<br> <a href="http://www.iliasdiakonikolas.org/">Ilias Diakonikolas</a>,
         <a href="https://cseweb.ucsd.edu/~dakane/">Daniel M. Kane</a>,
         <a href="https://pasin30055.github.io/">Pasin Manurangsi</a>, and Lisheng Ren
  	<br> <i> Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS 2022)</i>
    <br> Selected for <b> Oral Presentation </b>
  </li>
</ol>

<h2 style="clear: both;">Teaching and Service</h2>
<ul style="list-style-type: none">
  <li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, Spring 2022</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Algorithms, UW-Madison, Spring 2021</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Artificial Inteligence, UW-Madison, Fall 2020</li>
  <li style="margin: 0px">Teaching Assistant for Introduction to Computer Engineering, UW-Madison, Fall 2019</li>
  <li style="margin: 10px 0"><b>Reviewer:</b> AISTATS 2022, COLT 2022, NeurIPS 2023, FOCS 2024, NeurIPS 2025</li>
</ul>
</body>
<html>
